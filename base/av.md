# 视频基础

## 图像

### 像素与PPI

像素（Pixel，缩写PX）是图像显示的基本单位。

我们通常说一幅图片的大小，例如是 1920×1080，就是宽度为 1920 个像素点，高度为1080 个像素点。乘积是 2,073,600，也就是说，这个图片是两百万像素的。1920×1080，也被称为这幅图片的分辨率。

PPI，就是“Pixels Per Inch”，每英寸像素数。即手机（或显示器）屏幕上每英寸面积，到底能放下多少个“像素点”。这个值当然是越高越好，PPI 越高，图像就越清晰细腻。

<img src="assets/av/image-20210530162443370.png" alt="image-20210530162443370" style="zoom:33%;" />

以前的功能机，例如诺基亚，屏幕 PPI 都很低，有很强烈的颗粒感。

### RGB 颜色

任何颜色，都可以通过红色（Red）、绿色（Green）、蓝色（Blue）按照一定比例调制出来。这三种颜色，被称为“三原色”

在计算机里，R、G、B 也被称为“基色分量”。它们的取值，分别从 0 到 255，一共 256 个等级（256 是 2 的 8 次方）。所以，任何颜色，都可以用 R、G、B 三个值的组合表示。

通过这种方式，一共可以表达256×256×256=16,777,216 种颜色，因此也简称为 1600 万色。
RGB 三色，每色有 8bit，这种方式表达出来的颜色，也被称为 24 位色（占用 24bit）。
这个颜色范围已经超过了人眼可见的全部色彩，所以又叫真彩色。再高的话，对于我们人眼来说，已经没有意义了，完全识别不出来。

采样的位深有：8,16,32。

衡量视频，最主要的一个指标参数，就是帧率（Frame Rate）。在视频中，一个帧（Frame）就是指一幅静止的画面。帧率，就是指视频每秒钟包括的画面数量（FPS，Frames per second）。帧率越高，视频就越逼真、越流畅。

### YUV 信号

简单来说，YUV 就是另外一种颜色数字化表示方式。**视频通信系统之所以要采用 YUV，而不是 RGB，主要是因为 RGB 信号不利于压缩**。在 YUV 这种方式里面，加入了亮度这一概念。在最近几十年中，视频工程师发现，眼睛对于亮和暗的分辨要比对颜色的分辨更精细一些，也就是说，人眼对色度的敏感程度要低于对亮度的敏感程度。所以，工程师认为，在我们的视频存储中，没有必要存储全部颜色信号。我们可以把更多带宽留给黑—白信号（被称作“亮度”），将稍少的带宽留给彩色信号（被称作“色度”）。于是，就有了 YUV。

YUV 里面的“Y”，就是亮度（Luma），“U”和“V”则是色度（Chroma）。偶尔会见到的 Y'CbCr，也称为 YUV，是 YUV 的压缩版本，不同之处在于 Y'CbCr用于数字图像领域，YUV 用于模拟信号领域，MPEG、DVD、摄像机中常说的 YUV 其实就是 Y'CbCr。

- Y：亮度，就是灰度值。除了表示亮度信号外，还含有较多的绿色通道量。
- U：蓝色通道与亮度的差值。
- V：红色通道与亮度的差值。

<img src="assets/av/image-20210530164451235.png" alt="image-20210530164451235" style="zoom:50%;" />

<img src="assets/av/image-20220224160442109.png" alt="image-20220224160442109" style="zoom:80%;" />

![img](assets/av/24439730_13282389538k8V.jpg)

黑点表示采样该像素点的Y分量，以空心圆圈表示采用该像素点的UV分量

- YUV 4:4:4采样，每一个Y对应一组UV分量。彩色分量和亮度分量具有同样的水平和垂直清晰度。
- YUV 4:2:2采样，每两个Y共用一组UV分量。 彩色分量和亮度分量具有同样的清晰度，但水平清晰度彩色分量是亮度分量的一半。
- YUV 4:2:0采样，每四个Y共用一组UV分量。在水平和垂直清晰度方面，彩色分量都是亮度分量的一半。

**通常用的是 YUV4:2:0 的采样方式,能获得 1/2 的压缩率。这些预处理做完之后,就是正式的编码了**。

### HSV 人类视觉系统

HSV对应色调，饱和度，明度。

- 对高频信息不敏感
-  对高对比度更敏感
-  对亮度信息比色度信息更敏感
-  对运动的信息更敏感

针对 HSV 的特点，数字视频系统的设计应该考虑哪些因素？

- 丢弃高频信息，只编码低频信息
- 提高边缘信息的主观质量
-  降低色度的解析度
-  对感兴趣区域（Region of Interesting，ROI）进行特殊处理

YUV 色彩空间是指，Y：亮度分量，UV：两个色度分量。YUV 能更好的反映 HVS 特点，主流的编解码标准的压缩对象都是 YUV 图像。

### RGB 空间转化到 YUV 空间

```c
Y = 0.299 R + 0.587 G + 0.114 B
  约 0.3R + 0.6G + 0.1B 
```

### 未经编码的视频数据量有多大？

有了视频之后，就涉及到两个问题：

- 一个是存储
- 二个是传输

以一个分辨率 1920×1080，帧率 30 的视频为例：

共：1920×1280=2,073,600（Pixels 像素），每个像素点是 24bit（前面算过），也就是每幅图片 2073600×24=49766400 bit，8 bit（位）= 1 byte（字节），所以：49766400bit = 6220800byte≈6.22MB。

这是一幅 1920×1080 图片的原始大小，再乘以帧率 30，也就是说，每秒视频的大小是 186.6MB，每分钟大约是 11GB，一部 90 分钟的电影，约是1000GB。

如果按照 100Mbps 的网速（12.5MB/s），下载那部电影，需要 22 个小时。于是，必须要对视频进行编码。

**编码的终极目的，说白了，就是为了压缩。**

各种五花八门的视频编码方式，都是为了让视频变得体积更小，有利于存储和传输。

### 视频从录制到播放的整个过程

<img src="assets/av/image-20210530163744977.png" alt="image-20210530163744977" style="zoom:50%;" />

```
采集 -> A/D 转换 -> YUV 转换 -> 量化 -> 压缩编码 -> 封装 -> 传输
                                                        ↓
播放 <- 解码 <- 解封装 <- 解协议 <-  <-  <-  <-  <-  <-  <- 
```



### 帧

- I 帧：帧内编码帧，大多数情况下 I 帧就是关键帧，就是一个完整帧，无需任何辅助
  就能独立完整显示的画面。
- B 帧：帧是双向预测帧。参考前后图像帧编码生成。需要前面的 I/P 帧或者后面的
  P 帧来协助形成一个画面。
- P 帧：前向预测编码帧。是一个非完整帧，通过参考前面的 I 帧或 P 帧生成画面。

场是电视的概念，当计算机在显示器上播放视频时，它只会显示一系列完整的帧，而不使用交错场的电视技巧。不需要对场有太多的了解。

<img src="assets/av/image-20220224161328531.png" alt="image-20220224161328531" style="zoom:50%;" />

### 比特率/码率

比特率即码率，在不同领域有不同的含义，在多媒体领域，指单位时间播放音频或视频的比特数，可以理解成吞吐量或带宽。单位为 bps , 即 bits per second，每秒传输的数据量，常用单位有：kbps、mbps 等。

计算公式：码率（kbps）= 文件大小（kb）/ 时长（s）

通俗一点理解就是取样率，取样率越大，精度就越高，图像质量越好，但数据量也越大，所以要找到一个平衡点：用最低的比特率达到最少的失真。

在一个视频中，不同时段画面的复杂程度是不同的，比如高速变化的场景和几乎静止的场景，所需的数据量也是不同的，若都使用同一种比特率是不太合理的，所以引入了动态比特率。

动态比特率简称为 VBR，即 Variable Bit Rate，比特率可以随着图像复杂程度的不同而随之变化。

图像内容简单的片段采用较小的码率，图像内容复杂的片段采用较大的码率，这样既保证了播放质量，又兼顾了数据量的限制。比如 RMVB 视频文件，其中的 VB 就是指 VBR，表示采用动态比特率编码方式，达到播放质量与体积兼得的效果。

静态比特率简称为 CBR，即 Constant Bit Rate，比特率恒定。图像内容复杂的片段质量不稳定，图像内容简单的片段质量较好。上面列出的计算公式显然是针对 CBR ，除 VBR 和 CBR 外，还有 CVBR（Constrained VariableBit Rate） 、ABR (Average Bit Rate) 等等。

### 图像增强（ Image Enhancement）

图像增强作为一种重要的图像处理技术，目的无非就是两个： 第一更适合人眼的感觉；第二有利于后续的分析处理。

图像增强主要包括直方图均衡、平滑滤波、中值滤波、锐化等内容。一般情况下，图像增强既可以在空间域实现，也可以在频域内实现。 这里我们主要介绍在空间域内对图像进行点运算，它是一种既简单又重要的图像处理技术，它能让用户改变图像上像素点的灰度值，这样通过点运算处理将产生一幅新图像。总之，图像增强后，有利于视觉的效果和后续的处理，消除了相关性和高频噪
声，有利于图像的压缩和处理，节省带宽。

#### 图像平滑

主要目的是为了消除图像采样系统的质量因素所产生的噪声。图像的能量主要集中在其低频部分，噪声所在的频段主要在高频段，同时图像中的细节信息也主要集中在其高频部分，因此， 如何去掉高频干扰又同时保持细节信息是关键。

在空域法中，图像平滑的常用方法是采用均值滤波或中值滤波。

- 对于均值滤波，它是用一个有奇数点的滑动窗口在图像上滑动，将窗口中心点对应的图像像素点的灰度值用窗口内的各个点的灰度值的平均值代替，如果滑动窗口规定了在取均值过程中窗口各个像素点所占的权重，也就是各个像素点的系数，这时候就称为加权均值滤波；
- 对于中值滤波，对应的像素点的灰度值用窗口内的中间值代替。

在频域法中，一般采用低通滤波法。

#### 图像锐化

图像的边缘信息在图像风险和人的视觉中都是非常重要的，物体的边缘是以图像局部特性不连续的形式出现的。前面介绍的图像滤波对于消除噪声是有益的，但往往使图像中的边界、轮廓变的模糊，为了减少这类不利效果的影响，这就需要利用图像鋭化技术，使图像的边缘变得更加鲜明。图像銳化处理的目的就是为了使图像的边缘、轮廓线以及图像的细节变的清晰，经过平滑的图像变得模糊的根本原因是因为图像受到了平均或积分造成的，因此可以对其进行逆运算（如微分运算）就可以使图像变的清晰。从频率域来考虑，图像模糊的实质是因为其高频分量被衰减，因此可以用高通滤波器来使图像清晰。

#### 直方图均衡

图像直方图描述了一幅图像的灰度级内容，从数学上来说图像直方图是图像各灰度值统计特性与图像灰度值的函数，它统计一幅图像中各个灰度级出现的次数或概率；从图形上来说，它是一个二维图，横坐标表示图像中各个像素点的灰度级，纵坐标为各个灰度级上图像各个像素点出现的次数或概率。

直方图修正，就是通过一个灰度映射函数 S=F(r)，将原灰度直方图改造成你所希望的直方图。所以，直方图修正的关键就是灰度映射函数。直方图均衡化是一种最常用的直方图修正。它是把给定图像的直方图分布改造成均匀直方图分布。由信息学的理论来解释，具有最大熵(信息量)的图像为均衡化图像。

#### 白平衡

在早晨旭日初升时，我们看一个白色的物体，感到它是白的；而我们在夜晚昏暗的灯光下，看到的白色物体，感到它仍然是白的。这是由于人类从出生以后的成长过程中，人的大脑已经对不同光线下的物体的彩色还原有了适应性。但是，图像传感器没有这种人眼的适应性，在不同的光线下，由于图像传感器输出的不平衡性，造成其输出的彩色失真：或者图像偏蓝，或者偏红。如下图所示：

<img src="assets/av/image-20220225103022314.png" alt="image-20220225103022314" style="zoom:67%;" />

理解白平衡，涉及到另一个重要的概念：色温。所谓色温，简而言之，就是定量地以开尔文温度表示色彩。 **色温越高， 蓝色成分就越多； 色温越低， 红色成分就越多。**在摄影、摄像时，不同色温光源下拍摄物体，获得的图像不可避免会出现色彩上的偏差。为了很获得现实际世界中各种色彩的图像，必须消除环境中光源色温的影响，即进行白平衡处理。

### JPEG标准

参考ch4，P73



## 视频

### 空间采样与时间采样

数字视频可以理解为自然场景空间和时间的数字采样表示。

空间采样的主要技术指标为：分辨率(Resolution)

![image-20220224160729616](assets/av/image-20220224160729616.png)

时间采样的主要技术指标为：帧率

![image-20220224160753019](assets/av/image-20220224160753019.png)

### 视频文件格式

Windows 系统中的文件名都有后缀，例如 1.doc，2.wps，3.psd 等等。Windows 设置后缀名的目的是让系统中的应用程序来识别并关联这些文件，让相应的文件由相应的应用程序打开。所以常见的如 1.avi，2.mpg 这些都叫做视频的文件格式，它由你电脑上安装的视频播放器关联。

### 视频封装格式

AVI，MPEG，VOB 等是一种视频封装格式，相当于一种储存视频信息的容器。

**封装：就是封装格式，简单来说，就是将已经编码压缩好的视频轨和音频轨按照一定的格式放到一个文件中。**

封装格式也称多媒体容器Container，它只是为多媒体编码提供了一个“外壳”，也就是将所有的处理好的视频、音频或字幕都包装到一个文件容器内呈现给观众，这个包装的过程就叫封装。

<img src="assets/av/image-20210530174144724.png" alt="image-20210530174144724" style="zoom: 33%;" />

- AVI 格式（后缀为.AVI）：它的英文全称为 Audio Video Interleaved，即音频视频交错格式。它于 1992 年被 Microsoft 公司推出。 这种视频格式的优点是图像质量好。由于无损 AVI 可以保存 alpha 通道，经常被我们使用。缺点太多，体积过于庞大，而
  且更加糟糕的是压缩标准不统一。
- DV-AVI 格式（后缀为.AVI）：DV 的英文全称是 Digital Video Format，是由索尼、松下、JVC 等多家厂商联合提出的一种家用数字视频格式。
- MPEG 格式（文件后缀可以是 .MPG .MPEG .MPE .DAT .VOB .ASF .3GP .MP4等）：它的英文全称为 Moving Picture Experts Group，即运动图像专家组格式，该专家组建于 1988 年，专门负责为 CD 建立视频和音频标准，而成员都是为视频、音频及系统领域的技术专家。MPEG 文件格式是运动图像压缩算法的国际标准。MPEG 格式目前有三个压缩标准，分别是 MPEG－1、MPEG－2、和 MPEG－4。MPEG－1、MPEG－2 目前已经使用较少，着重介绍 MPEG－4，其制定于 1998 年，MPEG－4 是为了播放流式媒体的高质量视频而专门设计的，以求使用最少的数据获得最佳的图像质量。目前 MPEG-4 最有吸引力的地方在于它能够保存接近于 DVD 画质的小体积视频文件。
- Flash Video 格式（后缀为.FLV）：由 Adobe Flash 延伸出来的的一种流行网络视频封装格式。随着视频网站的丰富，这个格式已经非常普及。

#### MP4格式分析

![image-20210604001245606](assets/av/image-20210604001245606.png)

### 视频编码格式

所谓视频编码方式就是指能够对数字视频进行压缩或者解压缩（视频解码）的程序或者设备。通常这种压缩属于有损数据压缩。

也可以指通过特定的压缩技术，将某个视频格式转换成另一种视频格式。常见的编码方式有：

#### H.26X 系列 

它由 ITU[国际电传视讯联盟]主导，包括 H.261、H.262、H.263、H.264、H.265。

- H.261：主要在老的视频会议和视频电话产品中使用。
- H.263：主要用在视频会议、视频电话和网络视频上。
- **H.264**：H.264/MPEG-4 第十部分，或称 AVC（Advanced Video Coding，高级视频编码），是一种视频压缩标准，一种被广泛使用的高精度视频的录制、压缩和发布格式。
- **H.265**：高效率视频编码（High Efficiency Video Coding，简称 HEVC）是一种视频压缩标准，H.264/MPEG-4 AVC 的继任者。HEVC 被认为不仅提升图像质量，同时也能达到H.264/MPEG-4 AVC两倍之压缩率（等同于同样画面质量下比特率减少了50%），可支持 4K 分辨率甚至到超高画质电视，最高分辨率可达到 8192×4320（8K 分辨率），这是目前发展的趋势。直至 2013 年，Potplayer 添加了对于 H.265 视频的解码，尚未有大众化编码软件出现。

#### MPEG 系列

由 ISO国际标准组织机构下属的 MPEG运动图象专家组]开发。视频编码方面主要有：

- MPEG-1 第二部分（MPEG-1 第二部分主要使用在 VCD 上，有些在线视频也使用这种格式。该编解码器的质量大致上和原有的 VHS 录像带相当。
- MPEG-2 第二部分（MPEG-2 第二部分等同于 H.262，使用在 DVD、SVCD 和大多数数字视频广播系统和有线分布系统（cable distribution systems）中。）
- MPEG-4 第二部分（MPEG-4 第二部分标准可以使用在网络传输、广播和媒体存储上。比起 MPEG-2 和第一版的 H.263，它的压缩性能有所提高。）
- MPEG-4 第十部分（MPEG-4 第十部分技术上和 ITU-TH.264 是相同的标准，有时候也被叫做“AVC”）最后这两个编码组织合作，诞生了 H.264/AVC 标准。ITU-T 给这个标准命名为 H.264，而 ISO/IEC 称它为 MPEG-4 高级视频编码（Advanced Video
  Coding，AVC）。

视频的编码格式才是一个视频文件的本质所在，不要简单的通过文件格式和封装形式来区分视频。

**视频文件的封装格式并不影响视频的画质，影响视频画面质量的是视频的编码格式。**

![preview](assets/av/v2-c82daac9a76c4cf112b2566c3797290a_r.jpg)

![image-20210601232253806](assets/av/image-20210601232253806.png)

# 视频压缩编码的基本原理

## 预测编码

预测法是最简单和实用的视频压缩编码方法，**这时压缩编码后传输的并不是像素本身的取样幅值，而是该取样的预测值和实际值之差。**

![image-20220225135206675](assets/av/image-20220225135206675.png)

例如，同一帧内邻近像素，当前像素为 X，其左邻近像素为 A，上邻近像素为 B，上左邻近像素为 C 等。显然与 X 之间的距离近的像素，如 A 和 B 与 X 的相关性强，愈远相关性愈弱，如 C、 D、E、 F 等像素。以 P 作为预测值，按与 X 的距离不同给以不同的权值，把这些像素的加权和作为 X的预测值，与实际值相减，得到差值 q。由于临近像素之间相关性强， q 值非常小，达到压缩编码的目的。

接收端把差值 q 与预测值（事先已定义好，比当前 X 早到达接收端像素，如Ａ）相加，恢复原始值 X。归纳如下：

编码端： X－ A = q

解码端： q＋A = X

![image-20220225135803792](assets/av/image-20220225135803792.png)

其中， x（ n）为当前像素的实际值， p（ n）为其预测值， d（ n）为差值或残差值。该差值经量化后得到残差量化值 q(n)。预测值 p(n)经预测器得到，预测器输入为已存储在预测器内前面的各像素，和当前值，它们的加权和即为下一个预测器输出。由图 3.2 可见， 解码输出 x’(n)与原始信号 x(n)之间有个因量化而产生的量化误差。

【个人理解】上图的量化是对实际值与预测值的差值进行量化，实际值与预测值的差值为d(n)，q(n)是对差值d(n)进行量化后的残差量化值。从d(n)到q(n)是有误差的，但误差值很小。量化器的动态范围可以缩小，相应的量化分层数目就可减少，每个像素的编码比特数也显著下降，而且不致视频质量明显降低，达到视频压缩的目的。

参考下面的图，更能加深理解。

### 帧内预测编码

![image-20220225141305283](assets/av/image-20220225141305283.png)

#### 预测编码的量化器

一般说图像中平坦区域比突变区域多得多，例如人脸中，只有眼睛、鼻子、嘴等少量地方细节出现，其余则为平坦或缓变区域。

人眼视觉特性实验表明，**在亮度突变部分或变化大的部分，量化误差大些不会使人眼敏感，可采取粗量化，量化节距可取大一些，这时虽然需多些比特数，但面积小总比特数不大；反之，在亮度变化缓慢区域，则应取细量化，但由于平坦区域 e(x,y)小，也不会增加很多比特数**。总之，利用人眼这种掩盖效应采用非线性（不均匀）量化，可使总码率有所下降。表 3.1 为量化器输入输出值举

例。由于误差信号值有正有负(-3，-6，...，-150)，总量化级数为 19。

![image-20220225142907965](assets/av/image-20220225142907965.png)

由于量化，预测编码会产生过载、颗粒噪声、伪轮廓以及边沿忙乱等。它们都是由于量化值过小或不够小及像素变化过快等跟不上变化造成的。

### 帧间预测编码

一般而言，帧间预测编码编码效率比帧内更高。

#### 单向预测

##### 预测原理

<img src="assets/av/image-20220225144503614.png" alt="image-20220225144503614" style="zoom: 80%;" />

##### 基于块匹配算法的运动矢量估计

上述原理以像素为单位进行预测，除了传送帧差外，还增加了每个像素的运动矢量，编码效率显著下降。实际上，两帧之差的物体运动一般是刚体的平移运动，位移量不大，因此往往**把一帧图像分成若干 M×N 块，以块为单位分配运动矢量，大大降低总码率。**

![image-20220225145145895](assets/av/image-20220225145145895.png)

如上图所示，白色块为下一帧的块，它相对于当前帧位移为`d(i，j)`，则`d(i，j)`为运动矢量MV。

找到与当前帧块相匹配的前一帧块准则与算法，略。

#### 双向预测

前向参考帧预测当前帧称为前向运动补偿，利用后向参考帧预测当前帧称为后向运动补偿，利用前后向同时预测的就称为双向预测运动补偿。

**双向预测在实时通信中是不能应用的，例如会议电视、可视电话等，因为后向预测在当前帧之后进行，会引入编码时延。**它可用在广播电视系统中，如采用 MPEG 标准的编码系统， 特别针对一些暴露区域，即 t-1 帧尚未暴露而 t+1 帧已呈现出来的区域。

![image-20220225152428832](assets/av/image-20220225152428832.png)

由图可知，B帧的解码依赖前面的I和后面的P。

#### 重叠块运动补偿 OBMC 相对于基于块匹配的运动补偿的优点

基于块的运动补偿从计算量上看是比较简单，但这种人为的块划分使得每个块由一个运动矢量，容易产生方块效应，特别是运动矢量估计不准确或物体运动非简单的平移运动及一个块中有几个不同运动物体时。 OBMC 方法解决了运动矢量估计不准确的方法。
采用 OBMC 时，一个像素的预测不仅基于它所属的 MV 估计，还基于其相邻的 MV 估计。

#### 运动估计

在帧间预测编码中，由于活动图像邻近帧中的景物存在着一定的相关性。因此，可将活动图像分成若干块或宏块，并设法搜索出每个块或宏块在邻近帧图像中的位置，并得出两者之间的空间位置的相对偏移量，**得到的相对偏移量就是通常所指的运动矢量，得到运动矢量的过程被称为运动估计。**

##### 为什么通常使用基于块的运动表示法？

最直接和不受约束的方法是在每个像素都指定运动矢量，这就是所谓基于像素表示法。 这种表示法是对任何类型图像都是适用的，但是它需要估计大量的未知量，并且它的解时常在物理上是不正确，除非在估计过程中施加适当的物理约束。这在具体实现时是不可能的，通常采用基于块的物体运动表示法。

##### 基于块的运动表示法

一般对于细节比较少、比较平坦的区域选择块尺寸大一些，对于图像中细节比较多的区域选择块尺寸小一些。

宏块中的每个色度块（ Cb 和 Cr） 尺寸宽高都是亮度块的一半， 色度块的分割方法和亮度块同样，只是尺寸上宽高都是亮度块一半（如亮度块是 8×16 块尺寸大小，那么色度块就是 4×8，如果亮度块尺寸为 8×4，那么色度块便是 4×2 等等）。每个色度块的运动矢量的水平和垂直坐标都是亮度块的一半。

##### 运动矢量空间域预测方式

###### 运动矢量中值预测

![image-20220225160516224](assets/av/image-20220225160516224.png)

利用与当前块 E 相邻的左边块 A，上边块 B 和右上方的块 C 的运动矢量，取其中值来作为当前块的预测运动矢量。

设 E 为当前宏块、宏块分割或者亚宏块分割， A 在 E 的左侧， B 在 E 的上方、 C 在 E 的右上方，如果 E 的左侧多于一个块，那么选择最上方的块作为 A，在 E 的上方选择最左侧的块作为 B。

图 3.18 表示所有的块尺寸相同，图 3.19 表示邻近块尺寸不同时作为预测 E 的运动矢量的块的选择。

H.264 标准中提供的块尺寸有 16×16， 8×16， 16×8， 8×8， 8×4， 4×8， 4×4，它们的图象分割区域分别定义为搜索模式 Mode1-Mode7。

【注】剩下的3种预测方法不看了

空间域的预测更为准确，其中上层块预测的性能最优，因为其充分利用了不同预测块模式运动矢量之间的相关性。而中值预测性能随着预测块尺寸的减小而增加，这是因为当前块尺寸越小，相关性越强。

## 变换编码

平坦区域和内容缓慢变化区域占据一幅图像的大部分，而细节区域和内容突变区域则占小部分。也可以说，图像中直流和低频区占大部分，高频区占小部分。这样，**空间域的图像变换到频域或所谓的变换域，会产生相关性很小的一些变换系数，并可对其进行压缩编码，即所谓的变换编码。**

正交变换中的K-L 变换、离散余弦变换（ DCT） 可用于变换编码。其中，编码编码性能以 K-L 变换最理想，但缺乏快速算法，且变换矩阵随图像而异，不同图像需计算不同的变换矩阵，因而只用来参考比较。 **DCT 编码性能最接近于 K-L 变换，略次而已，具有快速算法，广泛应用于图像编码**。

对于图像变换编码， 最理想的变换操作应对整个图像进行，以便去除所有像素间的相关性。 但这样的操作计算量太大。 实际上， 往往把图像分为若干块， **以块为单位进行 DCT 变换**。 通常是  16×16 或 8×8 组成小块。

图像块通常只需用几个低频 DCT 系数表示。

变换系数量化后，在低频和直流区域少量较大的值，高频区域由少量不大的值，系数大部分为零，为了更有效的编码，通常根据该统计特性采用熵编码进一步压缩码率。

### 变换编码与预测编码的比较

变换编码实现比较复杂，预测编码的实现相对容易，但预测编码的误差会扩散。 以一行为例，由于后面像素以前面像素为参考，前面像素的预测误差会逐步向后面像素扩散。而且在二维预测时，误差会扩散至后面几行，形成区域误码。这样一来， 信道误码率要求提高。

变换编码则不会误码扩散，其影响只限制在一个块内，而且反变换后误码会均匀分散到块内各个像素上，对视觉无甚影响。

现实中，往往采用混合编码方法， 即对图像先进行带有运动补偿的帧间预测编码， 再对预测后残差信号进行 DCT 变换。这种混合编码方法已成为许多视频压缩编码国际标准的基本框架。

## 熵编码

利用信源的统计特性进行码率压缩的编码就称为熵编码，也叫统计编码。

视频编码常用的有两种：变长编码（也称哈夫曼编码）及算术编码。

### 变长编码

哈夫曼提出变长编码方法：对出现概率大的符号分配短字长的二进制码，对出现概率小的符号分配长字长二进制码，得到符号平均码长最短的码。变长编码也称最佳编码。具体编码方法如图：

<img src="assets/av/image-20220225164254631.png" alt="image-20220225164254631" style="zoom:67%;" />

哈夫曼编码的任意一个码字，都不可能是其他码字的前缀。因此通过哈夫曼编码的信息可以**紧密排列连续传输，而不用担心解码时的歧义性**。

哈弗曼编码可以参考 [【H.264/AVC视频编解码技术详解】七、 熵编码算法（1）：基础知识](https://yinwenjie.blog.csdn.net/article/details/52301584)

### 算术编码

算术编码和哈夫曼编码不同，不采用一个码字代表一个输入信息符号的办法，而采用一个浮点数来代替一串输入符号。

具体，略

## 三种压缩编码方式的总结

- 预测编码：传输预测像素值与实际像素值之差，利用时间或空间相邻像素之间较强的相关性
	- 帧内预测：预测值与实际值位于同一帧内，用于消除图像的空间冗余
	- 帧间预测：实际值位于当前帧，预测值位于参考帧，用于消除图像的时间冗余，相比帧内预测压缩率更高，由于帧间预测参考了其他帧的数据，因此，不能独立解码，所以只能在获取参考帧数据之后，才能重建当前帧
- 变换编码：对视频造成有限的可以容忍的损失来获取相对更高的编码效率，造成信息损失的部分，就在其变换量化的部分，在进行量化之前，先将像素由空间域变换到频域，针对变换系数进行编码
	- 可用于视频变换编码的正交变换：DCT变换、K-L变换
- 熵编码：熵编码用于消除视频信息中的冗余，由于信源中每一个符号出现的概率并不一致，导致使用同一长度的编码表示符号可能会造成浪费，通过熵编码，利用信源的统计特性进行压缩编码，可消除由于符号概率导致的冗余
	- 在视频编码算法中常用的熵编码方法有变长编码和算术编码等，具体来说主要有上下文自适应的变长编码(CAVLC)和上下文自适应的二进制算术编码(CABAC)

# H.264/AVC 编码器原理

## 档次和级

H.264 **不仅具有优异的压缩性能，而且具有良好的网络亲和性**，这对实时的视频通信是十分重要的(视频通信、流媒体)。

和 MPEG-4 中的重点是灵活性不同， H.264 着重在压缩的高效率和传输的高可靠性，因而其应用面十分广泛，具体说来， H.264 支持三个不同档次，每个档次支持一组特定的编码功能，并支持一类特定的应用：

1. 基本档次：利用 I 片和 P 片支持帧内和帧间编码，支持利用基于上下文的自适应的变长编码进行的熵编码（ CAVLC）。主要用于可视电话、会议电视、无线通信等实时视频通信；
2. 主要档次：支持隔行视频，采用 B 片的帧间编码和采用加权预测的帧内编码。支持利用基于上下文的自适应的算术编码（ CABAC）。主要用于数字广播电视与数字视频存储；
3. 扩展档次：支持码流之间有效的切换（ SP 和 SI 片）、改进误码性能（数据分割），但不支持隔行视频和 CABAC。主要用于网络的视频流，如视频点播；

## H.264编码器特点

H.264 并不明确地规定一个编解码器如何实现，而是规定了一个编了码的视频比特流的句法，和该比特流的解码方法， 各个厂商的编码器和解码器在此框架下应能够互通，在实现上具有较大灵活性。

![image-20220228093959563](assets/av/image-20220228093959563.png)

由于视频内容时刻在变化，有时空间细节很多，有时大面积的平坦。这种内容的多变性就必须采用相应的自适应的技术措施；由
于信道在环境恶劣下也是多变的，例如互联网，有时畅通，有时不畅，有时阻塞，又如无线网络，有时发生严重衰落，有时衰耗很小，这就要求采取相应的自适应方法来对抗这种信道畸变带来的不良影响。这两方面的多变带来了自适应压缩技术的复杂性。**H.264 就是利用实现的复杂性获得压缩性能的明显改善**。

## Ｈ.264编码数据格式按功能分为两层

H.264 的功能分为两层，即视频编码层（ VCL）和网络提取层（ NAL， Network Abstraction Layer）。 VCL 数据即编码处理的输出，它表示被压缩编码后的视频数据序列。在 VCL 数据传输或存储之前，这些编码的 VCL 数据，先被映射或封装进 NAL 单元中。

每个 NAL 单元包括一个原始字节序列负荷（ RBSP）、一组对应于视频编码数据的 NAL 头信息。

![image-20220228151336363](assets/av/image-20220228151336363.png)

## 帧内预测

预测块 P 是基于已编码重建块和当前块形成的。对亮度像素而言， P 块用于 4×4 子块或者 16×16 宏块的相关操作。 4×4 亮度子块有 9 种可选预测模式，独立预测每一个 4×4亮度子块，适用于带有大量细节的图像编码； 16×16 亮度块有 4 种预测模式，预测整个 16×16 亮度块，适用于平坦区域图像编码；**色度块也有 4 种预测模式，类似于 16×16 亮度块预测模式**。编码器通常选择使 P 块和编码块之间差异最小的预测模式。

### 4 × 4 亮度预测模式

![image-20220228152517985](assets/av/image-20220228152517985.png)

![image-20220228153749460](assets/av/image-20220228153749460.png)

之后会从这９种预测中，选出误差最小的模式作为编码方式。

### 16 × 16 亮度预测模式

![image-20220228154055151](assets/av/image-20220228154055151.png)

宏块的16 × 16亮度预测模式适合平坦区域。

可以参考博客[【H.264/AVC视频编解码技术详解】十六：帧内预测编码的基本原理](https://yinwenjie.blog.csdn.net/article/details/77203414)

## 帧间预测

### 树状结构运动补偿

每个宏块（ 16×16 像素）可以 4 种方式分割：一个 16×16，两个 16×8，两个 8×16，四个 8×8。其运动补偿也相应有四种。而 8×8 模式的每个子宏块还可以四种方式分割：一个 8×8，两个 4×8 或两个 8×4 及 4 个 4×4。这些分割和子宏块大大提高了各宏块之间的关联性。这种分割下的运动补偿则称为树状结构运动补偿。

![image-20220228154756454](assets/av/image-20220228154756454.png)

每个分割或子宏块都有一个独立的运动补偿。**每个 MV 必须被编码、传输，分割的选择也需编码到压缩比特流中。对大的分割尺寸而言， MV 选择和分割类型只需少量的比特，但运动补偿残差在多细节区域能量将非常高。小尺寸分割运动补偿残差能量低，但需要较多的比特表征 MV 和分割选择。分割尺寸的选择影响了压缩性能**。 整体而言，大的分割尺寸适合平坦区域，而小尺寸适合多
细节区域。

宏块的色度成分（ Cr 和 Cb）则为相应亮度的一半（水平和垂直各一半）。 色度块采用和亮度块同样的分割模式，只是尺寸减半（水平和垂直方向都减半）。例如， 8×16 的亮度块相应色度块尺寸为 4×8， 8×4 亮度块相应色度块尺寸为 4×2 等等。色度块的 MV 也是通过相应亮度 MV 水平和垂直分量减半而得。

举例：图 6.21 显示了一个残差帧（没有进行运动补偿）。 H.264 编码器为帧的每个部分选择了最佳分割尺寸，使传输信息量最小，并将选择的分割加到残差帧上。在帧变化小的区域（残差显示灰色），选择 16×16 分割；多运动区域（残差显示黑色或白色），选择更有效的小的尺寸。

![image-20220228155052897](assets/av/image-20220228155052897.png)



# 视频编码

原始视频压缩的目的是去除冗余信息，可以去除的冗余包括：

- 空间冗余：图像相邻像素之间有较强的相关性
- 时间冗余：视频序列的相邻图像之间内容相似
- 编码冗余：不同像素值出现的概率不同
- 视觉冗余：人的视觉系统对某些细节不敏感
- 知识冗余：规律性的结构可由先验知识和背景知识得到

## 无损压缩

压缩前、解压缩后图像完全一致 X=X'，压缩比低(2:1~3:1)。典型格式例如：Winzip，JPEG-LS。

## 有损压缩

压缩前解压缩后图像不一致 X≠X'，压缩比高(10:1~20:1)，利用人的视觉系统的特性。典型格式例如：MPEG-2，H.264/AVC，AVS。

## 编码定义

定义：通过特定的压缩技术，将某个视频格式的文件转换成另一种视频格式。

视频数据在时域和空域层面都有极强的相关性，这也表示有大量的时域冗余信息和空域冗余信息，压缩技术就是去掉数据中的冗余信息。

### 去除时域冗余信息

- 运动补偿：通过先前的局部图像来预测、补偿当前的局部图像，可有效减少帧序列冗余信息。
- 运动表示：不同区域的图像使用不同的运动矢量来描述运动信息，运动矢量通过熵编码进行压缩（熵编码在编码过程中不会丢失信息）。
- 运动估计：从视频序列中抽取运动信息。 通用的压缩标准使用基于块的运动估计和运动补偿。

### 去除空域冗余信息

- 变换编码：将空域信号变换到另一正交矢量空间，使相关性下降，数据冗余度减小。
- 量化编码：对变换编码产生的变换系数进行量化，控制编码器的输出位率。
- 熵编码：对变换、量化后得到的系数和运动信息，进行进一步的无损压缩。

<img src="assets/av/image-20210530185248779.png" alt="image-20210530185248779" style="zoom: 50%;" />

<img src="assets/av/image-20210530185305844.png" alt="image-20210530185305844" style="zoom:50%;" />

<img src="assets/av/image-20210530185325490.png" alt="image-20210530185325490" style="zoom: 50%;" />

视频编码技术优先消除的目标，就是空间冗余和时间冗余。

## H.264编码介绍

### IPB 帧

I 帧：帧内编码帧(intra picture)，采用帧内压缩去掉空间冗余信息。是自带全部信息的独立帧，是最完整的画面（占用的空间最大），无需参考其它图像便可独立进行解码。视频序列中的第一个帧，始终都是 I 帧。

P 帧：前向预测编码帧(predictive-frame)，通过将图像序列中前面已经编码帧的时间冗余信息来压缩传输数据量的编码图像。参考前面的 I 帧或者 P 帧。P 帧对前面的 P 和 I 参考帧有依赖性。但是，P 帧压缩率比较高，占用的空间较小。

B 帧：双向预测内插编码帧(bi-directional interpolated prediction frame)，既考虑源图像序列前面的已编码帧，又顾及源图像序列后面的已编码帧之间的冗余信息，来压缩传输数据量的编码图像，也称为双向编码帧。参考前面一个的 I 帧或者 P 帧及其后面的一个 P 帧。所以，它的压缩率最高，可以达到 200:1。**不过，因为依赖后面的帧，所以不适合实时传输（例如视频会议）**。

对帧的分类处理可以大大压缩视频的大小。毕竟，要处理的对象,大幅减少了(从整个图像,变成图像中的一个区域)。

<img src="assets/av/image-20220224164235440.png" alt="image-20220224164235440" style="zoom: 67%;" />

### PTS 和 DTS

- DTS(Decoding Time Stamp)是标识读入内存中 bit 流在什么时候开始送入解码器中进行解码。也就是解码顺序的时间戳。
- PTS(Presentation Time Stamp)用于度量解码后的视频帧什么时候被显示出来。在没有B 帧的情况下，DTS 和 PTS 的输出顺序是一样的，一旦存在 B 帧，PTS 和 DTS 则会不同。也就是显示顺序的时间戳。

### GOP

即 Group of picture(图像组)，指两个 I 帧之间的距离，Reference(参考周期)指两个 P 帧之间的距离。

一个 I 帧所占用的字节数大于一个 P 帧，一个 P 帧所占用的字节数大于一个 B 帧。所以在码率不变的前提下，GOP 值越大，P、B 帧的数量会越多，平均每个 I、P、B 帧所占用的字节数就越多，也就更容易获取较好的图像质量；Reference 越大，B 帧的数量越多，同理也更容易获得较好的图像质量。

简而言之：

字节大小：I > P > B

解码顺序：I -> P -> B

<img src="assets/av/image-20210530185050130.png" alt="image-20210530185050130" style="zoom:50%;" />

<img src="assets/av/image-20210530185902834.png" alt="image-20210530185902834" style="zoom:33%;" />

<img src="assets/av/image-20210530185922590.png" alt="image-20210530185922590" style="zoom:50%;" />

### 运动估计和补偿

<img src="assets/av/image-20220224164439978.png" alt="image-20220224164439978" style="zoom:67%;" />

人在动，但背景没动。左边是I帧，右边是P帧。差值为：

<img src="assets/av/image-20220224164545876.png" alt="image-20220224164545876" style="zoom:67%;" />

也就是说,图中的部分像素,进行了移动。移动轨迹如下：

<img src="assets/av/image-20220224164610392.png" alt="image-20220224164610392" style="zoom:67%;" />

这就是运动估计和补偿：

<img src="assets/av/image-20220224164711090.png" alt="image-20220224164711090" style="zoom:67%;" />

### 块(Block)与宏块(MacroBlock)

如果总是按照像素来算,数据量会比较大,所以,一般都是把图像切割为不同的“块(Block)”或“宏块(MacroBlock)”,对它们进行计算。

一个宏块一般为 16 像素×16 像素。

![image-20220225165839868](assets/av/image-20220225165839868.png)

块==》宏块(MB)==》片(Slice)==》片组==》图像(picture)

宏块（Macro Block）：是H.264编码的基本单位，一个编码图像首先要划分成多个块（4x4 像素）才能进行处理，显然宏块应该是整数个块组成，通常宏块大小为16x16个像素。

宏块分为I、P、B宏块：

I宏块只能利用当前片中已解码的像素作为参考进行帧内预测；

P宏块可以利用前面已解码的图像作为参考图像进行帧内预测；

B宏块则是利用前后向的参考图形进行帧内预测

## 帧内编码

<img src="assets/av/image-20220224164940961.png" alt="image-20220224164940961" style="zoom:67%;" />

大致过程如下：

- RGB 转 YUV:固定公式
- 图片宏块切割:宏块 16x16
- DCT:离散余弦变换
- 量化:取样
- ZigZag 扫描:
- DPCM:差值脉冲编码调制
- RLE:游程编码
- 霍夫曼编码:
- 算数编码:

# 视频解码

## 软解与硬解性能对比

软解码性能比硬解码差，指的是普通嵌入式系统，cpu主频才一个G，而现在的cpu是8核2G，所有它的解码性能是强于硬解码。且兼容性更强。

硬解码是写死的，比如只能是60帧/s。它的好处是它是固化的程序，不需要做指令转换，开销少，热供电小，不耗电。

如果没有特殊的要求，比如对解码速率要求很高，比如200帧/s，就只能用软解码。而对于正常视频播放，硬解码也无妨。



# 音频

音频数据的承载方式最常用的是脉冲编码调制，即 PCM。

在自然界中，声音是连续不断的，是一种模拟信号。声音是一种波，有自己的振幅和频率，要保存声音，就要保存声音在各个
时间点上的振幅。

PCM 的采集步骤分为以下步骤: 模拟信号 -> 采样 -> 量化 -> 编码 -> 数字信号。

奈奎斯特采样定理 : 为了不失真地恢复模拟信号,采样频率应该不小于模拟信号频谱中最高频率的 2 倍。

音调--->频率

音量--->振幅

音色--->不规则的正弦波（主音与泛音（环境声音）的叠加），即波形。

## 采样率

即采样的频率。

采样率要大于原声波频率的 2 倍，人耳能听到的最高频率为 20kHz，为了满足人耳的听觉要求，采样率至少为 40kHz，通常为 44.1kHz，更高的通常为 48kHz。

人耳听觉频率范围 [20Hz, 20KHz]

### 采样位数

采样位数也叫采样大小或者量化位数。

波形振幅在模拟信号上是连续的样本值，在数字信号中，要对其进行量化，只能取一个近似的值。为了记录这些振幅，采样器会采用一个固定的位数。通常有8位、16位、32位。位数越多,量化后的波形越接近原始波形，声音的质量越高,而需要的存储空间也越多。

<img src="assets/av/image-20220224152431509.png" alt="image-20220224152431509" style="zoom:80%;" />

## PCM音频参数

- 采样率sample_rate = 44100 (CD音质)

<img src="assets/av/image-20210604000148936.png" alt="image-20210604000148936" style="zoom: 50%;" />

样本大小

- AV_SAMPLE_FMT_S16：16位存储采样
- AV_SAMPLE_FMT_FLTP：一个float是32位，用浮点来存采样。用浮点来运算，效率更高

样本类型planar

- AV_SAMPLE_FMT_S16：在内存的格式是c1,c2,c1,c2,c1,c2。双声道交错
- AV_SAMPLE_FMT_S16P：在内存的格式是c1,c1,c1,...,c2,c2,c2...。先第一通道，后第二通道



## 通道数

**单声道**：单声道的声音只能使用一个扬声器发声，也可以让两个扬声器输出同一个声道的声音。当两个扬声器回放单声道信息的时候，我们可以明显感觉到声音是从两个音箱中间传递到我们耳朵里的，无法判断声源的具体位置。

**双声道**：双声道就是有两个声音通道，其原理是人们听到声音时可以根据左耳和右耳对声音相位差来判断声源的具体位置。声音在录制过程中被分配到两个独立的声道，从而达到了很好的声音定位效果。

记录声音时，如果每次生成一个声波数据，称为单声道；每次生成两个声波数据，称为双声道(立体声)。立体声(双声道)存储大小是单声道文件的两倍。

## 音频帧

音频跟视频不太一样，视频的每一帧就是一副图像，但是因为音频是流式的，本身是没有一帧的概念的。而且有些时候确实没有办法说一帧怎么怎么样。比如对于 PCM 流来说，采样率为 44100Hz，采样位数为 16，通道数为 2，那么一秒的音频固定大小的。44100 × 16 × 2 / 8 字节。但是人们可以规定一帧的概念，比如 amr 帧比较简单，它规定每 20ms 的音频是一帧。

## 音频码率

是指一个数据流中每秒钟能通过的信息量,单位 bps(bit per second)。

码率 = 采样率 * 采样位数 * 声道数

# 视频播放原理

## 视频播放通常步骤

视频播放器播放一个互联网上的视频文件（本地文件），需要经过以下几个步骤：

- 解协议
- 解封装
- 解码视音频
- 视音频同步

注意：“文件”本身也是一种“协议”。

### 解协议

将流媒体协议的数据，解析为标准的相应的封装格式数据。视音频在网络上传播的时候，常常采用各种流媒体协议，例如 HTTP，RTMP，或是 MMS 等等。

这些协议在传输视音频数据的同时，也会传输一些信令数据。这些信令数据包括对播放的控制（播放，暂停，停止），或者对网络状态的描述等。

**解协议的过程中会去除掉信令数据而只保留视音频数据**。例如，采用 RTMP 协议传输的数据，经过解协议操作后，输出 FLV 格式的数据。

### 解封装

**将输入的封装格式的数据，分离成为音频流压缩编码数据和视频流压缩编码数据**。封装格式种类很多，例如 MP4，MKV，RMVB，TS，FLV，AVI 等等，它的作用就是将已经压缩编码的视频数据和音频数据按照一定的格式放到一起。

例如，FLV 格式的数据，经过解封装操作后，输出 H.264 编码的视频码流和 AAC 编码的音频码流。

### 解码

**将视频/音频压缩编码数据，解码成为非压缩的视频/音频原始数据**。

音频的压缩编码标准包含 AAC，MP3，AC-3 等等，视频的压缩编码标准则包含 H.264，MPEG2，VC-1 等等。

解码是整个系统中最重要也是最复杂的一个环节。通过解码，压缩编码的视频数据输出成为非压缩的颜色数据，例如 YUV420P，RGB 等等；压缩编码的音频数据输出成为非压缩的音频抽样数据，例如 PCM 数据。

### 视音频同步

根据解封装模块处理过程中获取到的参数信息，同步解码出来的视频和音频数据，并将视频音频数据送至系统的显卡和声卡播放出来。